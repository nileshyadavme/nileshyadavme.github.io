<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://nileshyadavme.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nileshyadavme.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-22T18:18:35+05:30</updated><id>https://nileshyadavme.github.io/feed.xml</id><title type="html">Nilesh Y.</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">What is a Neural Network</title><link href="https://nileshyadavme.github.io/blog/2025/what-is-neural-network/" rel="alternate" type="text/html" title="What is a Neural Network"/><published>2025-08-22T18:59:10+05:30</published><updated>2025-08-22T18:59:10+05:30</updated><id>https://nileshyadavme.github.io/blog/2025/what-is-neural-network</id><content type="html" xml:base="https://nileshyadavme.github.io/blog/2025/what-is-neural-network/"><![CDATA[<p>Neural networks are at the heart of modern Artificial Intelligence. They are inspired by the way our brains process information, but instead of neurons and synapses, they use mathematical functions and weighted connections.</p> <h2 id="-the-basic-idea">ğŸ§  The Basic Idea</h2> <p>At its simplest, a neural network takes <strong>inputs</strong>, performs some transformations, and produces an <strong>output</strong>. Each transformation is controlled by numbers called <strong>weights</strong>, which the network learns during training.</p> <p>Think of it like this:</p> <ul> <li><strong>Input layer</strong> â†’ raw data (like pixels in an image)</li> <li><strong>Hidden layers</strong> â†’ where the magic happens (patterns and features are learned)</li> <li><strong>Output layer</strong> â†’ prediction or decision (like â€œdogâ€ vs â€œcatâ€)</li> </ul> <h2 id="ï¸-how-it-learns">âš™ï¸ How It Learns</h2> <p>Learning in a neural network is done through a process called <strong>backpropagation</strong>. The network makes a guess, checks how wrong it was, and then adjusts its weights slightly to do better next time. Repeat this thousands (or millions) of times, and the network gets pretty good at the task.</p> <h2 id="-real-world-applications">ğŸ“ Real-World Applications</h2> <p>Neural networks power many things we use every day:</p> <ul> <li>Image recognition (unlocking your phone with Face ID)</li> <li>Natural Language Processing (chatbots, translation)</li> <li>Recommendation systems (Netflix, Spotify)</li> <li>Autonomous vehicles</li> </ul> <h2 id="-why-its-exciting">âœ¨ Why Itâ€™s Exciting</h2> <p>What makes neural networks fascinating is their ability to <strong>learn patterns</strong> that humans might miss. Instead of writing explicit rules for every possible situation, we train the system on examples and let it figure things out.</p> <hr/> <p>ğŸ’¡ <em>This post is part of my ongoing series on machine learning basics. In upcoming articles, Iâ€™ll explore deeper architectures like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).</em></p>]]></content><author><name></name></author><category term="ml"/><category term="code"/><category term="math"/><category term="ml"/><category term="ai"/><summary type="html"><![CDATA[A simple introduction to neural networks â€” how they work, why they matter, and where theyâ€™re used.]]></summary></entry></feed>